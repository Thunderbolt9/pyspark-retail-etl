# ğŸ§¾ PySpark Retail ETL Pipeline

## ğŸ“Œ Overview
This project implements an **end-to-end batch ETL pipeline using PySpark** to process retail transaction data.  
The pipeline ingests raw CSV data, performs data cleaning and transformations, computes business-level aggregations, and stores the results in **optimized Parquet format**.

The project is designed to demonstrate **core Apache Spark concepts** such as:
- Lazy evaluation
- DAG execution
- Transformations vs Actions
- Aggregations and grouping
- Performance optimizations using caching

---

## ğŸ§  Business Use Case
Analyze retail transaction data to generate:
- **Daily revenue trends**
- **Top-selling product by revenue**

These insights help stakeholders understand sales performance and product contribution.

---

## ğŸ› ï¸ Tech Stack
- **Apache Spark (PySpark)**
- **Python 3**
- **Parquet (columnar storage)**

All tools and libraries used are **open-source and free**.

---

## ğŸ“‚ Project Structure
````markdown
pyspark-retail-etl/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.csv
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ daily-revenue/
â”‚   â””â”€â”€ top-selling-products/
â”‚
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ transform_retail_etl.py
â”‚
â””â”€â”€ README.md
